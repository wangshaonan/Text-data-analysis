---
title: Week 6, Nov. 9
---

### Pre-trained language model
- Why pre-trained language model
- Elmo
- GPT
- BERT
  

### Lecture
[Slides](https://pan.baidu.com/s/15rf8rp1aD9PLthnMDok9OQ) with code: eq29

### Reading
[Attention is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

[Deep Contextualized Word Representations](https://aclanthology.org/N18-1202.pdf)

[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
